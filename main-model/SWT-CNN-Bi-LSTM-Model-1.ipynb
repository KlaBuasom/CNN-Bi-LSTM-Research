{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#pip install python-binance\n",
    "#!pip install pandas\n",
    "#!pip install scikit-learn \n",
    "#!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Data\n",
    "apikey  = 'QHOA07WHc6ZkNS2A9i4bTcTbYuOpEqLI3eHu6iWp02gDOnmwMFpQsSQDO4nytISY'\n",
    "secret = 'yvHF7zGHB9CVj8cSHQb0j8cZdjhfNSdJ7FcWJuvIfVlNcCh4aT6UneTJRjcokzH9'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import warnings\n",
    "from termcolor import colored\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Autenticate\n",
    "client = Client(apikey, secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historical Data\n",
    "file_path = r'C:\\Users\\DJKlaKunG\\Desktop\\Personal_Document\\GitHub\\CNN-Bi-LSTM-Research\\training-data\\SWT-CNN-Bi-LSTM-Model-1\\hist_data.csv'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"Requestin Old Data\")\n",
    "    hist_df = pd.read_csv(file_path)\n",
    "    print(\"Requestin Old Data - Complete\")\n",
    "else:\n",
    "    print(\"Requestin New Data\")\n",
    "    historical = client.get_historical_klines('BTCUSDT', Client.KLINE_INTERVAL_15MINUTE, '1 Jan 2019')\n",
    "    hist_df = pd.DataFrame(historical)\n",
    "    hist_df.columns = ['Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close Time', 'Quote Asset Volume', \n",
    "                        'Number of Trades', 'TB Base Volume', 'TB Quote Volume', 'Ignore']\n",
    "    hist_df['Open Time'] = pd.to_datetime(hist_df['Open Time']/1000, unit='s')\n",
    "    hist_df['Close Time'] = pd.to_datetime(hist_df['Close Time']/1000, unit='s')\n",
    "    numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume', 'TB Base Volume', 'TB Quote Volume']\n",
    "    hist_df[numeric_columns] = hist_df[numeric_columns].apply(pd.to_numeric, axis=1)\n",
    "    print(\"Requestin New Data - Complete\")\n",
    "\n",
    "\n",
    "#Preprocessing Data\n",
    "hist_df.to_csv(file_path, index=False)\n",
    "print(\"Preprocessing Data - Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Nessary Value then appear in the CSV file\n",
    "warnings.filterwarnings('ignore')\n",
    "# 1. Calculate Gain and Loss\n",
    "hist_df['Gain'] = hist_df['Close'].diff().apply(lambda x: x if x > 0 else 0)\n",
    "hist_df['Loss'] = hist_df['Close'].diff().apply(lambda x: -x if x < 0 else 0)\n",
    "\n",
    "# 2. Calculate Average Gain and Average Loss for the last 14 bars\n",
    "hist_df['Avg_Gain_14'] = hist_df['Gain'].rolling(window=14).mean()\n",
    "hist_df['Avg_Loss_14'] = hist_df['Loss'].rolling(window=14).mean()\n",
    "\n",
    "# 3. Calculate ADX for 7 bars\n",
    "# First, we need to calculate the True Range, +DM, and -DM\n",
    "hist_df['HL'] = hist_df['High'] - hist_df['Low']\n",
    "hist_df['HPC'] = (hist_df['High'] - hist_df['Close']).shift(1)\n",
    "hist_df['LPC'] = (hist_df['Close'].shift(1) - hist_df['Low'])\n",
    "\n",
    "# True Range\n",
    "hist_df['TR'] = hist_df[['HL', 'HPC', 'LPC']].max(axis=1)\n",
    "\n",
    "# Positive Directional Movement (+DM) and Negative Directional Movement (-DM)\n",
    "hist_df['+DM'] = ((hist_df['High'] - hist_df['High'].shift(1)) > \n",
    "                 (hist_df['Low'].shift(1) - hist_df['Low'])).astype(int) * hist_df['HL']\n",
    "hist_df['-DM'] = ((hist_df['Low'].shift(1) - hist_df['Low']) > \n",
    "                 (hist_df['High'] - hist_df['High'].shift(1))).astype(int) * hist_df['HL']\n",
    "\n",
    "# Smoothed True Range and Directional Movements\n",
    "hist_df['Smoothed_TR'] = hist_df['TR'].rolling(window=7).sum()\n",
    "hist_df['Smoothed_+DM'] = hist_df['+DM'].rolling(window=7).sum()\n",
    "hist_df['Smoothed_-DM'] = hist_df['-DM'].rolling(window=7).sum()\n",
    "\n",
    "# Directional Indicators\n",
    "hist_df['+DI'] = 100 * hist_df['Smoothed_+DM'] / hist_df['Smoothed_TR']\n",
    "hist_df['-DI'] = 100 * hist_df['Smoothed_-DM'] / hist_df['Smoothed_TR']\n",
    "\n",
    "# DX (Directional Movement Index)\n",
    "hist_df['DX'] = 100 * (hist_df['+DI'] - hist_df['-DI']).abs() / (hist_df['+DI'] + hist_df['-DI'])\n",
    "\n",
    "# ADX (Average Directional Index)\n",
    "hist_df['ADX_7'] = hist_df['DX'].rolling(window=7).mean()\n",
    "\n",
    "# 4. Calculate EMA for 7 bars\n",
    "hist_df['EMA_7'] = hist_df['Close'].ewm(span=7, adjust=False).mean()\n",
    "\n",
    "# Drop intermediate columns used for calculations\n",
    "drop_cols = ['HL', 'HPC', 'LPC', 'TR', '+DM', '-DM', 'Smoothed_TR', 'Smoothed_+DM', 'Smoothed_-DM', '+DI', '-DI', 'DX']\n",
    "hist_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# Save the dataframe with calculated values to the specified file path\n",
    "hist_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Calculate Necessary data - Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Stationary Wavelet Transform (SWT)\n",
    "import pywt\n",
    "#Redefine the list of columns to transform\n",
    "columns_to_transform = ['Close', 'Gain', 'Loss', 'EMA_7','ADX_7']\n",
    "\n",
    "# Define the Fourier Transformation function again\n",
    "def apply_swt(series):\n",
    "    # Apply Stationary Wavelet Transform (SWT) to the series\n",
    "    coeffs = pywt.swt(series, wavelet='db1', level=1)\n",
    "    # We'll use the approximation coefficients for our analysis\n",
    "    return coeffs[0][0]\n",
    "\n",
    "# Apply SWT to the specified columns in the dataset\n",
    "for col in columns_to_transform:\n",
    "    hist_df[f'{col}_SWT'] = apply_swt(hist_df[col])\n",
    "\n",
    "# Save the dataframe with calculated values to the specified file path\n",
    "hist_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Calculate Necessary data - Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN-Bi-LSTM Model:\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "#Data Preparation: Shape the data into appropriate sequences for time series forecasting.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Hyperparameters #Parameters Settings\n",
    "SEQUENCE_LENGTH = 15\n",
    "LOOKBACK = 14\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "KERNEL_SIZE = 3\n",
    "FILTERS = 64\n",
    "POOL_SIZE = 2\n",
    "LSTM_UNITS1 = 50\n",
    "LSTM_UNITS2 = 50\n",
    "TEST_SIZE = 0.2\n",
    "ACTIVATION_FUNC = 'relu'\n",
    "LOSS_FUNC = 'mean_squared_error'\n",
    "PERFORMANCE_METRIC = RootMeanSquaredError(name='rmse')\n",
    "\n",
    "print(\"Setting New parameters - Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the transformed Close price as our feature\n",
    "feature_column = 'Close_SWT'\n",
    "target_column = 'Close_SWT'\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(hist_df[[feature_column]])\n",
    "\n",
    "# Create sequences of length 15 (14 features + 1 target)\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(scaled_data) - SEQUENCE_LENGTH):\n",
    "    X.append(scaled_data[i:i+SEQUENCE_LENGTH-1])\n",
    "    y.append(scaled_data[i+SEQUENCE_LENGTH-1])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "# Reshape the input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Modified model with an additional CNN layer\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First CNN layer\n",
    "model.add(Conv1D(filters=FILTERS, kernel_size=KERNEL_SIZE, activation=ACTIVATION_FUNC, input_shape=(LOOKBACK, 1)))\n",
    "model.add(MaxPooling1D(pool_size=POOL_SIZE))\n",
    "\n",
    "# Bi-LSTM layers\n",
    "model.add(Bidirectional(LSTM(LSTM_UNITS1, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(LSTM_UNITS2)))\n",
    "\n",
    "# Dense layer for prediction\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=LOSS_FUNC, metrics=[PERFORMANCE_METRIC])\n",
    "\n",
    "# Display the modified model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training In Progress..\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Training In Progress...\")\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total training time and average time per step\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_step = total_time / (len(X_train) // BATCH_SIZE * EPOCHS)  # Using '//' for integer division\n",
    "\n",
    "print(\"\\nTraining Completed!\")\n",
    "print(f\"Total Training Time: {total_time:.2f} seconds\")\n",
    "print(f\"Average Time Per Step: {avg_time_per_step:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Predicting the values for training and validation datasets\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Compute RMSE for training data\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "\n",
    "# Compute RMSE for validation data\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "train_rmse, test_rmse\n",
    "\n",
    "# Reconstruct the predictions and actual values to the original domain\n",
    "reconstructed_train_preds = scaler.inverse_transform(train_predictions)\n",
    "reconstructed_test_preds = scaler.inverse_transform(test_predictions)\n",
    "actual_train_values = scaler.inverse_transform(y_train)\n",
    "actual_test_values = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Compute RMSE for the reconstructed values\n",
    "reconstructed_train_rmse = np.sqrt(mean_squared_error(actual_train_values, reconstructed_train_preds))\n",
    "reconstructed_test_rmse = np.sqrt(mean_squared_error(actual_test_values, reconstructed_test_preds))\n",
    "\n",
    "print(\"Model Performance Evaluation:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Training RMSE: {train_rmse:.8f}\")\n",
    "print(f\"Validation RMSE: {test_rmse:.8f}\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Reconstructed Model Performance Evaluation:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Training RMSE: {reconstructed_train_rmse:.8f}\")\n",
    "print(f\"Validation RMSE: {reconstructed_test_rmse:.8f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Check if the CSV file exists and determine MODEL_NAME\n",
    "if os.path.exists(save_path):\n",
    "    existing_data = pd.read_csv(save_path)\n",
    "    model_number = len(existing_data) + 1\n",
    "    MODEL_NAME = f\"Model_{model_number:04d}\"  # Format the model number with leading zeros\n",
    "else:\n",
    "    MODEL_NAME = \"Model_0001\"\n",
    "\n",
    "# Update the model_path with the new MODEL_NAME\n",
    "model_path = r\"C:\\Users\\DJKlaKunG\\Desktop\\Personal_Document\\GitHub\\CNN-Bi-LSTM-Research\\model-data\\SWT-CNN-Bi-LSTM-Model-1\\\\\" + MODEL_NAME + \".h5\"\n",
    "# Save the model (assuming the model is already trained)\n",
    "model.save(model_path)\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    \"MODEL_NAME\": [MODEL_NAME],\n",
    "    \"SEQUENCE_LENGTH\": [SEQUENCE_LENGTH],\n",
    "    \"LOOKBACK\": [LOOKBACK],\n",
    "    \"EPOCHS\": [EPOCHS],\n",
    "    \"BATCH_SIZE\": [BATCH_SIZE],\n",
    "    \"LEARNING_RATE\": [LEARNING_RATE],\n",
    "    \"KERNEL_SIZE\": [KERNEL_SIZE],\n",
    "    \"FILTERS\": [FILTERS],\n",
    "    \"POOL_SIZE\": [POOL_SIZE],\n",
    "    \"LSTM_UNITS1\": [LSTM_UNITS1],\n",
    "    \"LSTM_UNITS2\": [LSTM_UNITS2],\n",
    "    \"ACTIVATION_FUNC\": [ACTIVATION_FUNC],\n",
    "    \"LOSS_FUNC\": [LOSS_FUNC],\n",
    "    \"PERFORMANCE_METRIC\": [str(PERFORMANCE_METRIC)],  # Convert metric to string for saving\n",
    "    \"TOTAL_TIME\": [total_time],\n",
    "    \"AVG_TIME_PER_STEP\": [avg_time_per_step],\n",
    "    \"TRAIN_RMSE\": [train_rmse],\n",
    "    \"TEST_RMSE\": [test_rmse],\n",
    "    \"RECONSTRUCTED_TRAIN_RMSE\": [reconstructed_train_rmse],\n",
    "    \"RECONSTRUCTED_TEST_RMSE\": [reconstructed_test_rmse],\n",
    "    \"TEST_SIZE\": [TEST_SIZE]\n",
    "}\n",
    "\n",
    "# Convert dictionary into a DataFrame\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Check if the CSV file exists and save data\n",
    "if os.path.exists(save_path):\n",
    "    df.to_csv(save_path, mode='a', header=False, index=False)  # Append data without header\n",
    "else:\n",
    "    df.to_csv(save_path, index=False)  # Save new file with header\n",
    "\n",
    "# Display the generated MODEL_NAME for verification\n",
    "\n",
    "\n",
    "print(\"Save - Complete \" + MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
