{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.10\n",
    "#pip install python-binance\n",
    "#pip install pandas\n",
    "#pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Data\n",
    "apikey  = 'QHOA07WHc6ZkNS2A9i4bTcTbYuOpEqLI3eHu6iWp02gDOnmwMFpQsSQDO4nytISY'\n",
    "secret = 'yvHF7zGHB9CVj8cSHQb0j8cZdjhfNSdJ7FcWJuvIfVlNcCh4aT6UneTJRjcokzH9'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import warnings\n",
    "from termcolor import colored\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Autenticate\n",
    "client = Client(apikey, secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historical Data\n",
    "file_path = r'C:\\Users\\DJKlaKunG\\Desktop\\Personal Document\\GitHub\\CNN-Bi-LSTM-Research\\training-data\\hist_data.csv'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"Requestin Old Data\")\n",
    "    hist_df = pd.read_csv(file_path)\n",
    "    print(\"Requestin Old Data - Complete\")\n",
    "else:\n",
    "    print(\"Requestin New Data\")\n",
    "    historical = client.get_historical_klines('BTCUSDT', Client.KLINE_INTERVAL_15MINUTE, '1 Jan 2019')\n",
    "    hist_df = pd.DataFrame(historical)\n",
    "    hist_df.columns = ['Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close Time', 'Quote Asset Volume', \n",
    "                        'Number of Trades', 'TB Base Volume', 'TB Quote Volume', 'Ignore']\n",
    "    hist_df['Open Time'] = pd.to_datetime(hist_df['Open Time']/1000, unit='s')\n",
    "    hist_df['Close Time'] = pd.to_datetime(hist_df['Close Time']/1000, unit='s')\n",
    "    numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume', 'TB Base Volume', 'TB Quote Volume']\n",
    "    hist_df[numeric_columns] = hist_df[numeric_columns].apply(pd.to_numeric, axis=1)\n",
    "    print(\"Requestin New Data - Complete\")\n",
    "\n",
    "\n",
    "#Preprocessing Data\n",
    "hist_df.to_csv(file_path, index=False)\n",
    "print(\"Preprocessing Data - Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Nessary Value then appear in the CSV file\n",
    "warnings.filterwarnings('ignore')\n",
    "# 1. Calculate Gain and Loss\n",
    "hist_df['Gain'] = hist_df['Close'].diff().apply(lambda x: x if x > 0 else 0)\n",
    "hist_df['Loss'] = hist_df['Close'].diff().apply(lambda x: -x if x < 0 else 0)\n",
    "\n",
    "# 2. Calculate Average Gain and Average Loss for the last 14 bars\n",
    "hist_df['Avg_Gain_14'] = hist_df['Gain'].rolling(window=14).mean()\n",
    "hist_df['Avg_Loss_14'] = hist_df['Loss'].rolling(window=14).mean()\n",
    "\n",
    "# 3. Calculate ADX for 7 bars\n",
    "# First, we need to calculate the True Range, +DM, and -DM\n",
    "hist_df['HL'] = hist_df['High'] - hist_df['Low']\n",
    "hist_df['HPC'] = (hist_df['High'] - hist_df['Close']).shift(1)\n",
    "hist_df['LPC'] = (hist_df['Close'].shift(1) - hist_df['Low'])\n",
    "\n",
    "# True Range\n",
    "hist_df['TR'] = hist_df[['HL', 'HPC', 'LPC']].max(axis=1)\n",
    "\n",
    "# Positive Directional Movement (+DM) and Negative Directional Movement (-DM)\n",
    "hist_df['+DM'] = ((hist_df['High'] - hist_df['High'].shift(1)) > \n",
    "                 (hist_df['Low'].shift(1) - hist_df['Low'])).astype(int) * hist_df['HL']\n",
    "hist_df['-DM'] = ((hist_df['Low'].shift(1) - hist_df['Low']) > \n",
    "                 (hist_df['High'] - hist_df['High'].shift(1))).astype(int) * hist_df['HL']\n",
    "\n",
    "# Smoothed True Range and Directional Movements\n",
    "hist_df['Smoothed_TR'] = hist_df['TR'].rolling(window=7).sum()\n",
    "hist_df['Smoothed_+DM'] = hist_df['+DM'].rolling(window=7).sum()\n",
    "hist_df['Smoothed_-DM'] = hist_df['-DM'].rolling(window=7).sum()\n",
    "\n",
    "# Directional Indicators\n",
    "hist_df['+DI'] = 100 * hist_df['Smoothed_+DM'] / hist_df['Smoothed_TR']\n",
    "hist_df['-DI'] = 100 * hist_df['Smoothed_-DM'] / hist_df['Smoothed_TR']\n",
    "\n",
    "# DX (Directional Movement Index)\n",
    "hist_df['DX'] = 100 * (hist_df['+DI'] - hist_df['-DI']).abs() / (hist_df['+DI'] + hist_df['-DI'])\n",
    "\n",
    "# ADX (Average Directional Index)\n",
    "hist_df['ADX_7'] = hist_df['DX'].rolling(window=7).mean()\n",
    "\n",
    "# 4. Calculate EMA for 7 bars\n",
    "hist_df['EMA_7'] = hist_df['Close'].ewm(span=7, adjust=False).mean()\n",
    "\n",
    "# Drop intermediate columns used for calculations\n",
    "drop_cols = ['HL', 'HPC', 'LPC', 'TR', '+DM', '-DM', 'Smoothed_TR', 'Smoothed_+DM', 'Smoothed_-DM', '+DI', '-DI', 'DX']\n",
    "hist_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# Save the dataframe with calculated values to the specified file path\n",
    "hist_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Calculate Necessary data - Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Stationary Wavelet Transform (SWT)\n",
    "import pywt\n",
    "#Redefine the list of columns to transform\n",
    "columns_to_transform = ['Close', 'Gain', 'Loss', 'EMA_7','ADX_7']\n",
    "\n",
    "# Define the Fourier Transformation function again\n",
    "def apply_swt(series):\n",
    "    # Apply Stationary Wavelet Transform (SWT) to the series\n",
    "    coeffs = pywt.swt(series, wavelet='db1', level=1)\n",
    "    # We'll use the approximation coefficients for our analysis\n",
    "    return coeffs[0][0]\n",
    "\n",
    "# Apply SWT to the specified columns in the dataset\n",
    "for col in columns_to_transform:\n",
    "    hist_df[f'{col}_SWT'] = apply_swt(hist_df[col])\n",
    "\n",
    "# Save the dataframe with calculated values to the specified file path\n",
    "hist_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Calculate Necessary data - Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation: Shape the data into appropriate sequences for time series forecasting.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the transformed Close price as our feature\n",
    "feature_column = 'Close_SWT'\n",
    "target_column = 'Close_SWT'\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(hist_df[[feature_column]])\n",
    "\n",
    "# Create sequences of length 15 (14 features + 1 target)\n",
    "X, y = [], []\n",
    "sequence_length = 15\n",
    "\n",
    "for i in range(len(scaled_data) - sequence_length):\n",
    "    X.append(scaled_data[i:i+sequence_length-1])\n",
    "    y.append(scaled_data[i+sequence_length-1])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape the input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN-Bi-LSTM Model:\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "# Hyperparameters\n",
    "LOOKBACK = 14\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0001\n",
    "KERNEL_SIZE = 3\n",
    "FILTERS = 64\n",
    "POOL_SIZE = 2\n",
    "LSTM_UNITS1 = 50\n",
    "LSTM_UNITS2 = 50\n",
    "ACTIVATION_FUNC = 'relu'\n",
    "LOSS_FUNC = 'mean_squared_error'\n",
    "PERFORMANCE_METRIC = RootMeanSquaredError(name='rmse')\n",
    "\n",
    "# Modified model with an additional CNN layer\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First CNN layer\n",
    "model.add(Conv1D(filters=FILTERS, kernel_size=KERNEL_SIZE, activation=ACTIVATION_FUNC, input_shape=(LOOKBACK, 1)))\n",
    "model.add(MaxPooling1D(pool_size=POOL_SIZE))\n",
    "\n",
    "# Bi-LSTM layers\n",
    "model.add(Bidirectional(LSTM(LSTM_UNITS1, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(LSTM_UNITS2)))\n",
    "\n",
    "# Dense layer for prediction\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=LOSS_FUNC, metrics=[PERFORMANCE_METRIC])\n",
    "\n",
    "# Display the modified model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training In Progress..\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Training In Progress...\")\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total training time and average time per step\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_step = total_time / (len(X_train) // BATCH_SIZE * EPOCHS)  # Using '//' for integer division\n",
    "\n",
    "print(\"\\nTraining Completed!\")\n",
    "print(f\"Total Training Time: {total_time:.2f} seconds\")\n",
    "print(f\"Average Time Per Step: {avg_time_per_step:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
